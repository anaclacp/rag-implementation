## Voc√™ Est√° Fazendo RAG Errado: Como Corrigir a Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) para LLMs Locais

**Como Configurar o RAG Localmente, Evitar Problemas Comuns e Melhorar a Precis√£o da Recupera√ß√£o do RAG.**

**DarkBones**
*Towards AI*

‚úîÔ∏è Quer ir direto para a configura√ß√£o? V√° para o tutorial.
‚úîÔ∏è Precisa de uma atualiza√ß√£o sobre RAG? Confira meu artigo anterior.

### RAG Funciona‚Ä¶ At√© Que N√£o Funciona

RAG parece √≥timo, at√© voc√™ tentar implement√°-lo. Ent√£o as rachaduras come√ßam a aparecer.

O RAG puxa peda√ßos irrelevantes, mistura ideias n√£o relacionadas e atribui confiantemente a escrita em primeira pessoa incorretamente, transformando o contexto √∫til em uma bagun√ßa confusa.

### Como Tornar o RAG Mais Inteligente

Enfrentei dois grandes problemas ao construir meu pr√≥prio sistema RAG:

*   **üß© Cegueira de Contexto** ‚Äî Quando os peda√ßos recuperados n√£o carregam informa√ß√µes suficientes para serem √∫teis.
*   **ü§¶ Confus√£o de Primeira Pessoa** ‚Äî Quando o sistema n√£o sabe a quem "Eu" se refere.

Vou mostrar exatamente como resolvi esses problemas, para que seu sistema RAG realmente entenda o que ele recupera.

Ao final, voc√™ ter√° um pipeline RAG 100% local, 100% gratuito e com reconhecimento de contexto, executado com seu LLM local e interface preferidos. Tamb√©m configuraremos uma base de conhecimento automatizada, para que adicionar novas informa√ß√µes seja simples.

### Antes de come√ßarmos‚Ä¶

Est√° gostando desta an√°lise detalhada? Veja como voc√™ pode ajudar:

*   üëè Aplaudir este artigo ‚Äî Isso ajuda mais pessoas a encontr√°-lo.
*   üîî Seguir-me ‚Äî Eu escrevo sobre IA, programa√ß√£o, ci√™ncia de dados e outras tecnologias interessantes. Mais posts como este est√£o por vir!
*   üí¨ Deixe um coment√°rio ‚Äî Voc√™ j√° lutou com RAG? Encontrou uma maneira melhor de dividir os dados? Vamos conversar!

### üß© Problema RAG #1: Cegueira de Contexto

Digamos que meu banco de dados de vetores contenha plots detalhados de duas s√©ries de filmes radicalmente diferentes: Matrix e O Senhor dos An√©is. Para simplificar, estamos usando pequenos peda√ßos sem sobreposi√ß√£o.

Cada plot √© dividido em frases e armazenado como embeddings em um banco de dados de vetores. Isso deve funcionar bem, certo? ‚Ä¶Certo?

### O Que Acontece Quando Consultamos o Banco de Dados?

Vamos ver o que acontece quando fazemos algumas perguntas.

**Entrada:** Quem forjou o Um Anel e por qu√™?

**Sa√≠da:**

```json
[
  "Sauron forjou o Um Anel na Montanha da Perdi√ß√£o para controlar a Terra-m√©dia.",
  "O Um Anel foi feito para escravizar Elfos, An√µes e Homens.",
  "Acredita-se que Neo seja Aquele que salvar√° a humanidade.",
  "Morpheus estava em uma miss√£o para encontrar Aquele.",
  "Aquele √© uma figura profetizada destinada a trazer equil√≠brio e acabar com a guerra."
]
```

**Entrada:** Quem ou o que era o Or√°culo em Matrix?

**Sa√≠da:**

```json
[
  "O Or√°culo √© um programa em Matrix que guia Neo em seu caminho."  ,
  "Galadriel, a Senhora de Lothl√≥rien, oferece sabedoria e previs√£o √† Sociedade.",
  "O Or√°culo ajuda os humanos a entenderem suas escolhas dentro da Matrix.",
  "Gandalf √© um Maia, enviado √† Terra-m√©dia para guiar e aconselhar seu povo.",
  "O Or√°culo prev√™ a vinda Daquele que acabar√° com a guerra."  
]
```

Agora, vamos alimentar este contexto para um LLM e ver o que ele gera.

**Entrada:**

Quem forjou o Um Anel e por qu√™?

**Resposta do LLM:**

O Um Anel foi forjado por Sauron e Agente Smith para controlar a Terra-m√©dia e escravizar Elfos, An√µes e Homens. Acredita-se que Neo seja Aquele que salvar√° a humanidade, ignorando as outras ra√ßas. Sauron enganou os Elfos para ajudar, enquanto o Agente Smith s√≥ queria destruir tanto humanos quanto a Matrix.

**Entrada:**

Quem ou o que era o Or√°culo em Matrix?

**Resposta do LLM:**

O Or√°culo √© um programa em Matrix que guia Neo em seu caminho, assim como Galadriel, a Senhora de Lothl√≥rien, que oferece sabedoria e previs√£o √† Sociedade. Juntos, eles preveem a vinda Daquele que acabar√° com a guerra, jogando a Matrix na Montanha da Perdi√ß√£o.

*(Fonte: Imagem gerada pelo autor com Flux.)*

### O Que Deu Errado?

Lixo entra, lixo sai.

O banco de dados de vetores n√£o √© inteligente, ele apenas classifica por similaridade.

Ele n√£o sabe o que √© um "filme" ou que Matrix e O Senhor dos An√©is s√£o hist√≥rias diferentes.

"Or√°culo" √© um termo vago, ent√£o ele tamb√©m recupera Galadriel.

O LLM aceita cegamente o contexto, misturando conceitos em um disparate.

Os peda√ßos recuperados carecem de um significado mais amplo, causando misturas irrelevantes e alucina√ß√µes.

Na verdade, poder√≠amos ter nos sa√≠do melhor sem dar nenhum contexto ao LLM.

Isto √© Cegueira de Contexto e √© por isso que as implementa√ß√µes RAG ing√™nuas muitas vezes fazem mais mal do que bem.

### ü§¶ Problema RAG #2: Confus√£o de Perspectiva em Primeira Pessoa

Eu escrevo muito na primeira pessoa. Artigos, documenta√ß√£o t√©cnica, prepara√ß√µes para palestras, e-mails e um di√°rio profissional rastreando meu trabalho e conquistas.

Mas minha base de conhecimento n√£o armazena apenas meus escritos. Tamb√©m cont√©m:

*   Artigos que salvei para mais tarde
*   Documenta√ß√£o para ferramentas que referencio
*   Outros recursos, todos misturados

Mas quando uma consulta recupera esses peda√ßos, como um LLM saberia qual deles √© realmente sobre mim?

*   "Eu projetei um sistema robusto de detec√ß√£o de anomalias de dados."
*   "Eu escalei o Monte Fuji no menor n√∫mero de passos."

Obviamente, √© o primeiro, mas um banco de dados de vetores n√£o tem como saber disso.

### üõ†Ô∏è Resolvendo a Cegueira de Contexto com Pedacos Sens√≠veis ao Contexto

Armazenar peda√ßos brutos n√£o √© suficiente. Mesmo os humanos lutam para entender frases isoladas sem contexto, ent√£o por que um banco de dados de vetores se sairia melhor?

Precisamos tornar cada peda√ßo consciente do contexto.

**Antes vs. Depois: Um Pedaco Mais Inteligente**

Aqui est√° um pedaco bruto do nosso exemplo:

`O Or√°culo √© um programa em Matrix que guia Neo em seu caminho.`

E aqui est√° uma vers√£o com reconhecimento de contexto:

```
<file_summary>
Este arquivo cont√©m uma explica√ß√£o detalhada da trilogia de filmes Matrix.
</file_summary>

<chunk_summary>
Este pedaco cont√©m detalhes sobre o personagem "O Or√°culo".
</chunk_summary>

<chunk_headers>
# Matrix, ## O Primeiro Filme, ### Personagens Not√°veis, #### O Or√°culo
</chunk_headers>

<chunk_content>
O Or√°culo √© um programa em Matrix que guia Neo em seu caminho.
</chunk_content>
```

Mesmo que voc√™ n√£o conhe√ßa Matrix ou O Or√°culo, isso esclarece sobre o que √© o peda√ßo.

### üîç Como Geramos Contexto?

Pedimos a um LLM para resumir o documento usando pedacos-chave:

*   Resumo do Arquivo -> Baseado na introdu√ß√£o e conclus√£o (onde o contexto chave geralmente reside).
*   Resumo do Pedaco -> Vinculado ao documento mais amplo.
*   Cabe√ßalhos -> Extra√≠dos do markdown.

Armazenamos tudo em um novo campo de banco de dados:

*   O campo `full_context` -> √â isso que vetorizamos e incorporamos.
*   O pedaco original (com cabe√ßalhos) -> √â isso que o banco de dados retorna.

### üéØ Por Que Isso Funciona

Ao incorporar o contexto, melhoramos a recupera√ß√£o:

*   ‚úÖ O sistema sabe sobre o que √© o pedaco.
*   ‚úÖ Ele recupera menos pedacos irrelevantes.
*   ‚úÖ O LLM obt√©m um contexto melhor, reduzindo as alucina√ß√µes.

Isto n√£o √© √∫til apenas para plots de filmes ‚Äî √© essencial para documentos legais, bases de conhecimento t√©cnico e IA empresarial.

Uma corre√ß√£o simples, mas uma melhoria massiva.

### üì¶ Por Que N√ÉO Retornar o Contexto?

O contexto adicionado s√≥ ajuda na recupera√ß√£o, o LLM n√£o precisa dele.

Para manter o processamento de arquivos r√°pido, eu uso um LLM mais leve para resumir os pedacos. Ele comete muitos erros, mas o banco de dados de vetores n√£o se importa. Ele ainda consegue identificar corretamente quais pedacos s√£o mais relevantes.

Mas se o LLM realmente l√™ esses erros? Agora √© ele quem est√° ficando confuso.

### Recupera√ß√£o Mais Inteligente, Respostas Mais Limpas

Em vez de alimentar o LLM com resumos confusos e propensos a erros, n√≥s:

*   ‚úÖ Retornamos apenas o pedaco original, n√£o o contexto gerado
*   ‚úÖ Minimizamos o uso de tokens e evitamos desperdi√ßar computa√ß√£o
*   ‚úÖ Aceleramos o processamento, pulando a re-sumariza√ß√£o desnecess√°ria

Dessa forma, a recupera√ß√£o se torna mais inteligente sem transformar a resposta em um desastre alucinado.

### ü§¶ Resolvendo a Confus√£o de Primeira Pessoa

A corre√ß√£o para isto √© simples, mas um pouco hacky.

Eu mantenho um diret√≥rio na minha base de conhecimento com meu primeiro nome. Se o processador de pedacos detecta um arquivo deste diret√≥rio (ou qualquer um de seus subdiret√≥rios), ele modifica o prompt para perguntar ao LLM sumarizador:

`Ele √© explicitamente instru√≠do a deixar abundantemente claro que este texto √© escrito por mim.`

Ele substitui refer√™ncias gen√©ricas como Eu, mim e meu pelo meu nome completo no resumo.

Antes de consultar o banco de dados de vetores, eu coloco meu nome completo no prompt.

`Jo√£o Silva: Qual √© meu RP nos 5k?`

Claro, n√£o √© a corre√ß√£o mais elegante, mas funciona. E em qualquer sistema, simples e eficaz vence complexo e fr√°gil.

Ao esclarecer a autoria na fase de sumariza√ß√£o, o sistema pode finalmente diferenciar entre minhas conquistas e aquele blog de viagens que salvei sobre escalar o Monte Fuji.

### üõ†Ô∏è Configurando Tudo

Para configurar este sistema localmente, voc√™ precisa dos seguintes componentes:

*   Banco de Dados -> PostgreSQL
*   Interface LLM Local -> Ollama
*   LLMs -> Eu recomendo qwen2.5 / deepseek-r1 / llama3, o que for adequado para suas necessidades e hardware
*   Incorporador (Embedder) -> nomic-embed-text, ou qualquer modelo de embedding que voc√™ preferir
*   UI para intera√ß√£o -> Eu recomendo open-webui
*   Banco de dados de vetores -> Supabase
*   Sistema RAG -> darkrag (o meu pr√≥prio)
*   Automa√ß√£o -> n8n, ou qualquer sistema similar de sua prefer√™ncia

Tudo √© executado em cont√™ineres Docker. Se voc√™ √© novo no Docker, confira alguns guias para iniciantes antes de prosseguir.

Eu executo meus cont√™ineres em uma rede Docker personalizada (ai-network) para que eles possam se comunicar sem configura√ß√£o manual.

Eu executo o Arch Linux, ent√£o criei daemons systemd em `~/systemd/.config/systemd/user/`, mas esta configura√ß√£o √© adapt√°vel para Windows, macOS e outras distribui√ß√µes Linux. Em vez de usar um arquivo docker-compose grande, eu executo os cont√™ineres separadamente para que eu possa habilitar/desabilitar componentes conforme necess√°rio para liberar mem√≥ria.

### üöÄ Configurando o Ollama e baixando LLMs

Execute o seguinte comando para iniciar o Ollama:

```bash
/usr/bin/docker run --rm \
    --network=ai-network \
    --name ollama \
    -v ollama_models:/root/.ollama/models \
    -p 11434:11434 \
    ollama/ollama:latest
```

**Nota:** O volume `ollama_models` √© mapeado para `/root/.ollama/models`, garantindo que os modelos persistam mesmo depois de desligar o cont√™iner.

Uma vez que o Ollama esteja em execu√ß√£o, entre no cont√™iner: `docker exec -it ollama /bin/bash`

Dentro do cont√™iner, puxe os modelos necess√°rios:

```bash
ollama pull qwen2.5
ollama pull nomic-embed-text
```

Ent√£o saia de volta para sua m√°quina local:

```bash
exit
```

**[Opcional] Executando o Ollama como um Daemon**

Se voc√™ quer que o Ollama inicie automaticamente quando voc√™ fizer login, crie um servi√ßo systemd ou o que for o equivalente para seu sistema operacional:

```
; systemd/.config/systemd/user/ollama.service
[Unit]
Description=Servidor de Modelo AI Ollama
After=default.target
BindsTo=default.target

[Service]
ExecStartPre=-/usr/bin/docker network create ai-network
ExecStart=/usr/bin/docker run --rm \
    --network=ai-network \
    --name ollama \
    -v ollama_models:/root/.ollama/models \
    -p 11434:11434 \
    ollama/ollama:latest
ExecStop=/usr/bin/docker stop ollama
ExecStopPost=/usr/bin/docker rm ollama
Restart=always
RestartSec=5

[Install]
WantedBy=default.target
```

Habilite-o com:

```bash
systemctl --user enable --now ollama
```

### üì¶ Configurando o PostgreSQL

Execute o seguinte comando para iniciar um cont√™iner PostgreSQL:

```bash
/usr/bin/docker run --rm \
    --network=ai-network \
    --name=postgres \
    -p 5432:5432 \
    -v ~/Apps/postgres:/var/lib/postgresql/data \
    -e POSTGRES_USER=user \
    -e POSTGRES_PASSWORD=password \
    -e POSTGRES_DB=default_db \
    postgres:16-alpine
```

Substitua `user` e `password` pelas credenciais desejadas.

### üñ•Ô∏è Configurando o Open-WebUI

Open-webui fornece uma UI semelhante ao ChatGPT para modelos locais.

*(Uma captura de tela da interface Open WebUI executando o modelo qwen2.5:7b. A UI tem um tema escuro, exibindo um prompt de chat central com op√ß√µes para "Pesquisa na Web" e "Interpretador de C√≥digo". Na barra lateral esquerda, h√° se√ß√µes rotuladas como "Workspace" e "Chats". A barra de endere√ßo do navegador mostra "N√£o Seguro" com um URL local. O canto superior direito exibe as configura√ß√µes do perfil do usu√°rio. Fonte: Captura de tela do autor. A interface pertence ao Open-WebUI.)*

Inicie-o com:

```bash
/usr/bin/docker run --rm \
    --network=ai-network \
    -e OLLAMA_BASE_URL=http://ollama:11434 \
    -e PORT=4080 \
    -p 4080:4080 \
    -v /path/to/your/conversations/on/your/machine:/app/backend/data \
    --name open-webui \
    ghcr.io/open-webui/open-webui:main
```

**Nota:** Substitua `/path/to/your/conversations/on/your/machine` pelo caminho real na sua m√°quina local onde voc√™ quer que o banco de dados resida.

**Configurando Fun√ß√µes Open-WebUI**

Navegue para `http://localhost:4080` e v√° para:

`Settings > Admin Panel > Functions`

Crie uma nova fun√ß√£o

Habilite-a clicando nos tr√™s pontos > Habilitar Global

Alternativamente, habilite-a por modelo em `Model Settings`

Use o seguinte script de fun√ß√£o:

`Open-Webui Webhook Function`

### üóÑÔ∏è Configurando o Supabase

Siga o guia oficial para auto-hospedar o Supabase com Docker.

Ent√£o, atualize seu arquivo `docker-compose.yml` para usar a `ai-network`:

`Supabase customized docker-compose`

**[Opcional] Executando o Supabase como um Daemon**

Para minha pr√≥pria configura√ß√£o, eu clonei o reposit√≥rio Supabase no meu diret√≥rio `~/Apps` e o executo com este daemon:

```
; systemd/.config/systemd/user/supabase.service
[Unit]
Description=Supabase
After=docker.service
BindsTo=default.target

[Service]
ExecStartPre=-/usr/bin/docker network create ai-network || true
ExecStart=/usr/bin/docker compose -f %h/Apps/supabase/docker/docker-compose.yml up
ExecStop=/usr/bin/docker compose -f %h/Apps/supabase/docker/docker-compose.yml down
Restart=always
RestartSec=5

[Install]
WantedBy=default.target
```

**Configurando o Supabase**

V√° para:

`http://localhost:8000`

Encontre seu nome de usu√°rio e senha em `.env` do reposit√≥rio Supabase.

No editor SQL, execute:

```sql
CREATE SEQUENCE documents_id_seq;

CREATE TABLE documents (
    id BIGINT PRIMARY KEY DEFAULT nextval('documents_id_seq'),
    content TEXT,
    metadata JSONB,
    embedding VECTOR(768),
    content_hash TEXT DEFAULT 'placeholder'::text,
    summary TEXT DEFAULT 'placeholder'::text,
    full_context TEXT DEFAULT 'placeholder'::text
);
```

Para habilitar a pesquisa vetorial, crie uma fun√ß√£o de similaridade:

```sql
create or replace function match_documents(
  query_embedding vector,
  match_count int
) returns table (
  id uuid,
  content text,
  metadata jsonb,
  similarity double precision
) language plpgsql as $$
begin
  return query
  select
    id,
    content,
    metadata,
    1 - (documents.embedding <=> query_embedding) as similarity
  from documents
  order by documents.embedding <=> query_embedding
  limit match_count;
end;
$$;
```

Tamb√©m precisamos de uma maneira de remover dados do banco de dados pela chave `"file_path"` nos metadados. Dessa forma, podemos evitar ter entradas duplicadas para os mesmos documentos. Execute isso no editor SQL para criar essa fun√ß√£o:

```sql
create or replace function delete_documents_by_path(
  target_path text
) returns void 
language plpgsql as $$
begin
  delete from documents 
  where metadata->>'file_path' = target_path;
end;
$$;
```

### üõ† Configurando o darkrag

Por padr√£o, o darkrag √© projetado para arquivos Markdown porque minha base de conhecimento √© principalmente notas estruturadas, documenta√ß√£o e artigos salvos. No entanto, se voc√™ trabalha com outros formatos, como PDFs, arquivos de texto simples ou mesmo p√°ginas da web, voc√™ pode facilmente modificar a l√≥gica de processamento de arquivos para suport√°-los.

Puxe a imagem darkrag:

```bash
docker pull darkbones/darkrag:latest
```

Crie um arquivo `.env` (por exemplo, `~Apps/darkrag/.env`):

```
# .env
DEFAULT_DATABASE_TABLE=documents
AUTHOR_NAME=John
AUTHOR_FULL_NAME="John Doe"
AUTHOR_PRONOUN_ONE=he
AUTHOR_PRONOUN_TWO=him

SUPABASE_URL=http://kong:8000
SUPABASE_KEY=your-supabase-key-as-found-in-your-supabase-env-file
OLLAMA_URL=http://ollama:11434
DEFAULT_MODEL=qwen2.5:7b
EMBEDDING_MODEL=nomic-embed-text:latest
```

**Vari√°veis importantes:**

*   `AUTHOR_NAME`: Para qualquer arquivo no diret√≥rio `AUTHOR_NAME`, ou qualquer um de seus subdiret√≥rios, darkrag solicitar√° ao sumarizador de pedacos que substitua todas as refer√™ncias em primeira pessoa como "Eu" ou "mim" pelo seu nome completo, para resolver o "problema de confus√£o de primeira pessoa". Por exemplo, se um arquivo em `seu-diret√≥rio-de-base-de-conhecimento/John/about-john.md` cont√©m "Eu gosto de trens", o sumarizador de pedacos adicionar√° algo como "John Doe gosta de trens" ao resumo contextualizado do pedaco.
*   `AUTHOR_FULL_NAME`: Seu nome completo para que o darkrag possa contextualizar pedacos em primeira pessoa.
*   `AUTHOR_PRONOUN_ONE` & `AUTHOR_PRONOUN_TWO`: Necess√°rio para o prompt para contextualizar pedacos em primeira pessoa.
*   `SUPABASE_KEY`: Necess√°rio para conectar √† inst√¢ncia Supabase. Voc√™ pode encontrar esta chave no seu arquivo `.env` do Supabase.
*   `DEFAULT_MODEL`: O LLM que ir√° resumir os pedacos. Eu recomendo `qwen2.5:7b`, pois √© leve e preciso o suficiente.

Substitua as vari√°veis com valores reais conforme necess√°rio.

Finalmente, execute este comando para executar o darkrag:

```bash
docker run --rm \
  --env-file [caminho-para-seu-arquivo-env] \
  --network=ai-network \
  --name=darkrag \
  -v /mnt/SnapIgnore/AI/knowledge:/data \
  -p 8004:8004 \
  darkbones/darkrag:latest
```

Apenas lembre-se de substituir `[caminho-para-seu-arquivo-env]` pelo caminho real para o seu arquivo `.env` que voc√™ criou acima. Se voc√™ preferir, voc√™ tamb√©m pode fornecer as vari√°veis de ambiente no comando `docker-run` diretamente se voc√™ preferir isso em vez de usar um arquivo `.env`.

### üì° Configurando o n8n para Automa√ß√£o de Workflow

Agora que temos todas as pe√ßas individuais, √© hora de conectar tudo.

Se voc√™ n√£o est√° familiarizado com o n8n, √© uma ferramenta de automa√ß√£o low-code que ajuda a integrar diferentes servi√ßos.

N√≥s o usaremos para:

*   Monitorar e atualizar a base de conhecimento
*   Garantir que as incorpora√ß√µes vetoriais permane√ßam atualizadas
*   Lidar com consultas RAG dinamicamente

**Executando o n8n como um Cont√™iner Docker**

Execute o seguinte comando para iniciar o n8n:

```bash
/usr/bin/docker run --rm \
    --network=ai-network \
    --name=n8n \
    -p 5678:5678 \
    -v ~/Apps/n8n:/home/node/.n8n \
    -v /mnt/SnapIgnore/AI/knowledge:/home/knowledge \
    -e N8N_BASIC_AUTH_ACTIVE=true \
    -e N8N_BASIC_AUTH_USER=admin \
    -e N8N_BASIC_AUTH_PASSWORD=suasenha \
    -e DB_TYPE=postgresdb \
    -e DB_POSTGRESDB_HOST=postgres \
    -e DB_POSTGRESDB_PORT=5432 \
    -e DB_POSTGRESDB_DATABASE=n8n \
    -e DB_POSTGRESDB_USER=user \
    -e DB_POSTGRESDB_PASSWORD=password \
    -e N8N_SECURE_COOKIE=false \
    n8nio/n8n:latest
```

**Nota:** Substitua `suasenha`, `user` e `password` pelos seus valores preferidos.

**Componentes importantes:**

*   `-v` mapeia um diret√≥rio na sua m√°quina local (pode ser qualquer diret√≥rio que voc√™ deseje) para o diret√≥rio `/home/knowledge` no n8n. Isso permite que o n8n veja os arquivos naquele diret√≥rio para que voc√™ possa atualizar automaticamente a base de conhecimento se voc√™ adicionar ou alterar qualquer arquivo neste diret√≥rio.
*   `N8N_BASIC_AUTH_USER`: altere isto para o nome de usu√°rio desejado para o n8n
*   `N8N_BASIC_AUTH_PASSWORD`: altere isto para a senha desejada para o n8n
*   `DB_POSTGRESDB_USER`: altere isto para o nome de usu√°rio que voc√™ configurou ao configurar o PostgreSQL
*   `DB_POSTGRESDB_PASSWORD`: altere isto para a senha que voc√™ configurou ao configurar o PostgreSQL

**Configurando as Credenciais do n8n**

Uma vez que o n8n esteja em execu√ß√£o, v√° para:

`http://localhost:5678`

Navegue para:

`Settings > Credentials`

Clique em `New Credential`

Adicione credenciais para Ollama e Supabase

*(Uma captura de tela da interface n8n mostrando a configura√ß√£o de uma credencial Ollama. A janela de configura√ß√£o de credenciais exibe uma mensagem de conex√£o bem-sucedida em verde, com o URL base definido como ‚Äúhttp://ollama:11434‚Äù. Um bot√£o ‚ÄúRetry‚Äù √© vis√≠vel, juntamente com um prompt para abrir a documenta√ß√£o para obter ajuda. O plano de fundo mostra o espa√ßo de trabalho n8n com op√ß√µes de barra lateral como ‚ÄúTemplates‚Äù, ‚ÄúVariables‚Äù e ‚ÄúHelp. Fonte: Captura de tela do autor. A interface pertence ao n8n.)*

*(Uma captura de tela da interface n8n mostrando a configura√ß√£o de uma credencial Supabase. A janela de configura√ß√£o de credenciais exibe uma mensagem de conex√£o bem-sucedida em verde, com o URL base definido como ‚Äúhttp://kong:8000‚Äù. Um bot√£o ‚ÄúRetry‚Äù √© vis√≠vel, juntamente com um prompt para abrir a documenta√ß√£o para obter ajuda. O plano de fundo mostra o espa√ßo de trabalho n8n com op√ß√µes de barra lateral como ‚ÄúTemplates‚Äù, ‚ÄúVariables‚Äù e ‚ÄúHelp. Fonte: Captura de tela do autor. A interface pertence ao n8n.)*

### üîÑ Automatizando as Atualiza√ß√µes da Base de Conhecimento no n8n

Configuraremos tr√™s workflows:

*   `Knowledge Base Updater` -> Atualiza o banco de dados quando os arquivos s√£o adicionados, alterados ou exclu√≠dos
*   `Knowledge Base Rebuilder` -> Garante periodicamente que todos os documentos sejam incorporados corretamente e estejam atualizados
*   `RAG Webhook` -> Lida com consultas do usu√°rio buscando pedacos de conhecimento relevantes

Vamos configur√°-los um por um.

### üìÇ Knowledge Base Updater

Este workflow monitora seu diret√≥rio de base de conhecimento e aciona atualiza√ß√µes sempre que os arquivos s√£o adicionados, alterados ou exclu√≠dos.

Ele tem dois conjuntos de tr√™s n√≥s:

*   Um conjunto para adi√ß√µes/atualiza√ß√µes de arquivos
*   Outro conjunto para exclus√µes de arquivos

*(Uma captura de tela do editor de workflow n8n exibindo uma automa√ß√£o "Knowledge Base Updater". O workflow consiste em dois processos paralelos: um acionado por um evento "File Updated" e o outro por um evento "File Deleted", ambos monitorando o diret√≥rio "/home/knowledge". Cada acionador est√° conectado a um n√≥ de processamento de c√≥digo, seguido por um n√≥ de solicita√ß√£o HTTP que interage com um sistema RAG em ‚Äúhttp://darkrag:8004". Fonte: Captura de tela do autor. A interface pertence ao n8n.)*

Este n√≥ monitora seu diret√≥rio de base de conhecimento:

*(Uma captura de tela do editor de workflow n8n exibindo a configura√ß√£o de um n√≥ "File Updated". O n√≥ est√° configurado para acionar em "Changes Involving a Specific Folder", monitorando o diret√≥rio "/home/knowledge" para eventos "File Added" e "File Changed". O painel direito mostra as configura√ß√µes dispon√≠veis, incluindo uma op√ß√£o para adicionar propriedades adicionais. Um bot√£o "Test Step" est√° vis√≠vel para testar manualmente o acionador. Fonte: Captura de tela do autor. A interface pertence ao n8n.)*

J√° que mapeamos nosso diret√≥rio de base de conhecimento para `/home/knowledge` no n8n, ele escuta as mudan√ßas de arquivo dentro desse caminho.

**Processando Caminhos de Arquivo**

O seguinte n√≥ JavaScript garante que os caminhos de arquivo sejam formatados corretamente antes de pass√°-los para o darkrag:

```javascript
const paths = [];
for (const item of $input.all()) {
  paths.push(item.json.path.replace(/^\/home\/knowledge\//, ""));
}

return { paths };
```

Ele remove `/home/knowledge/` do caminho do arquivo para que o darkrag receba apenas o caminho relativo.

**Enviando Solicita√ß√µes de Atualiza√ß√£o & Exclus√£o**

Dois n√≥s de solicita√ß√£o HTTP enviam os caminhos de arquivo processados para o darkrag:

*(Uma captura de tela do editor de workflow n8n mostrando a configura√ß√£o de um n√≥ "HTTP Request". A solicita√ß√£o √© definida para o m√©todo POST com o URL ‚Äúhttp://darkrag:8004/store/process_files" para enviar dados de atualiza√ß√£o de arquivo para o Darkrag. A autentica√ß√£o √© definida como ‚ÄúNone‚Äù, e o corpo da solicita√ß√£o est√° habilitado, formatado como JSON. A se√ß√£o ‚ÄúBody Parameters‚Äù inclui um par√¢metro chamado ‚Äúfile_paths‚Äù com um valor din√¢mico ‚Äú{{ $json.paths }}‚Äù. Op√ß√µes para par√¢metros e cabe√ßalhos adicionais est√£o dispon√≠veis. Fonte: Captura de tela do autor. A interface pertence ao n8n.)*

*(Uma captura de tela do editor de workflow n8n mostrando um n√≥ "HTTP Request" para excluir arquivos do Darkrag. A solicita√ß√£o usa o m√©todo POST para ‚Äúhttp://darkrag:8004/store/delete_files" com par√¢metros de corpo JSON. O par√¢metro ‚Äúfile_paths‚Äù √© definido dinamicamente como ‚Äú{{ $json.paths }}‚Äù. A autentica√ß√£o est√° desabilitada e os cabe√ßalhos est√£o desabilitados. Um bot√£o ‚ÄúTest Step‚Äù est√° dispon√≠vel para execu√ß√£o manual, e nenhum dado de entrada est√° presente no momento. Fonte: Captura de tela do autor. A interface pertence ao n8n.)*

### üìã Knowledge Base Rebuilder

Isto √© executado uma vez por semana para garantir que todo o conhecimento seja incorporado corretamente e esteja atualizado.

*(Uma captura de tela do editor de workflow n8n exibindo a automa√ß√£o "Knowledge Base Rebuilder". O fluxo come√ßa com um n√≥ "Schedule Trigger", seguido por dois n√≥s "HTTP Request" que interagem com o Darkrag. A primeira solicita√ß√£o limpa arquivos desatualizados, e a segunda processa a base de conhecimento atualizada. O workflow √© marcado como "Active", com op√ß√µes para editar, visualizar execu√ß√µes e compartilhar. A barra lateral esquerda inclui op√ß√µes de navega√ß√£o como "Templates", "Variables" e "Help." Fonte: Captura de tela do autor. A interface pertence ao n8n.)*

**Passo 1: Remover Entradas Obsoletas**

Ele primeiro exclui entradas de banco de dados para arquivos que n√£o existem mais:

*(Uma captura de tela do editor de workflow n8n mostrando um n√≥ "HTTP Request" configurado para limpar o banco de dados Darkrag. A solicita√ß√£o usa o m√©todo POST para ‚Äúhttp://darkrag:8004/store/clean_database" sem autentica√ß√£o. JSON est√° selecionado como o tipo de conte√∫do do corpo, mas nenhum par√¢metro de corpo √© especificado. Um bot√£o "Test Step" est√° dispon√≠vel para execu√ß√£o manual. Fonte: Captura de tela do autor. A interface pertence ao n8n.)*

**Passo 2: Reprocessar Arquivos Existentes**

Ent√£o, ele reprocessa todos os arquivos para garantir que as incorpora√ß√µes estejam atualizadas:

*(Uma captura de tela de um n√≥ n8n "HTTP Request" configurado para acionar o reprocessamento de todos os arquivos no Darkrag. O m√©todo de solicita√ß√£o √© POST, direcionando ‚Äúhttp://darkrag:8004/store/process_all" sem autentica√ß√£o. JSON est√° selecionado como o tipo de conte√∫do do corpo e um tempo limite de 900.000 milissegundos √© definido. Fonte: Captura de tela do autor. A interface pertence ao n8n.)*

### üñ•Ô∏è RAG Webhook

Este workflow habilita consultas RAG ao vivo buscando os pedacos de conhecimento mais relevantes do Supabase.

*(Uma captura de tela de um workflow n8n intitulado "RAG Webhook". O workflow come√ßa com um n√≥ Webhook, enviando a entrada para um n√≥ "Supabase Vector Store" que recupera incorpora√ß√µes com Ollama. O conte√∫do extra√≠do √© processado por um n√≥ de c√≥digo antes de ser enviado como uma resposta atrav√©s do n√≥ "Respond to Webhook". O workflow est√° ativo, com a chave de altern√¢ncia ligada. Fonte: Captura de tela do autor. A interface pertence ao n8n.)*

**Buscando Conhecimento do Supabase**

*(Uma captura de tela do n√≥ "Supabase Vector Store" em um workflow n8n, configurado para recuperar conhecimento relevante da tabela "documents". O prompt de consulta extrai dinamicamente a entrada usando {{$json.body.prompt}}, e o sistema recupera os 5 principais resultados correspondentes, incluindo metadados. A fun√ß√£o de consulta usada √© "match_documents." Fonte: Captura de tela do autor. A interface pertence ao n8n.)*

**Consultando o Ollama para Vetorizar o Prompt Original**

*(Uma captura de tela do n√≥ "Embeddings Ollama" em um workflow n8n, configurado para gerar incorpora√ß√µes vetoriais para consultas. O n√≥ est√° configurado para conectar com uma conta Ollama e usa o modelo "nomic-embed-text:latest" para transformar texto de entrada em incorpora√ß√µes para gera√ß√£o aumentada por recupera√ß√£o (RAG). Fonte: Captura de tela do autor. A interface pertence ao n8n.)*

**Preparando o Conhecimento Recuperado**

O n√≥ JavaScript formata o conte√∫do recuperado do darkrag em uma resposta estruturada:

```javascript
const knowledge = [];

for (const item of $input.all()) {
  const relPath = item.json.document.metadata.file_path.replace(/^\/home\/knowledge\//, ""));
  const content = item.json.document.pageContent;
  knowledge.push({
    file_path: relPath,
    content: content,
  });
}

return { knowledge }
```

**Processamento de Resposta Final**

A resposta darkrag formatada √© ent√£o retornada ao solicitante:

*(Uma captura de tela do n√≥ ‚ÄúRespond to Webhook‚Äù em um workflow n8n, configurado para retornar o conhecimento recuperado do darkrag. O n√≥ responde com JSON, usando a express√£o {{$json.knowledge}} para enviar os dados recuperados de volta ao solicitante. Fonte: Captura de tela do autor. A interface pertence ao n8n.)*

### üìå Conclus√£o

√â isso! Voc√™ agora tem um pipeline RAG totalmente funcional, autoatualiz√°vel, completamente local e gratuito para usar.

**Como Funciona na Pr√°tica**

‚úÖ Quando voc√™ consulta o modelo:

*   O Open-WebUI intercepta seu prompt e o envia para o n8n.
*   O n8n consulta o Supabase por pedacos relevantes.
*   O Supabase retorna os 5 pedacos mais relevantes.
*   O Open-WebUI injeta esses pedacos no seu prompt.
*   O LLM responde com melhor precis√£o.

‚úÖ Quando voc√™ adiciona, altera ou exclui documentos:

*   O n8n detecta a mudan√ßa e notifica darkrag.
*   O darkrag gera novas incorpora√ß√µes sens√≠veis ao contexto.
*   As incorpora√ß√µes atualizadas s√£o armazenadas no Supabase.

*(Uma captura de tela do Open-WebUI mostrando uma conversa com o modelo qwen2.5:7b. O usu√°rio pergunta: "Quem ou o que era o Or√°culo em Matrix?" O modelo responde com uma explica√ß√£o precisa, descrevendo o Or√°culo como um programa em Matrix que guia Neo, foi criado pelas m√°quinas para estudar o comportamento humano e √© conhecido por seus conselhos enigm√°ticos, mas perspicazes. A interface tem um tema escuro, com a conversa de bate-papo exibida no centro e uma barra lateral √† esquerda mostrando o hist√≥rico de bate-papo. Fonte: Captura de tela do autor. A interface pertence ao Open-WebUI.)*

### Considera√ß√µes Finais

"O darkrag resolve todos os problemas com o RAG?"

N√£o, o darkrag n√£o √© uma bala de prata. Ele n√£o responder√° de forma confi√°vel a perguntas como "O que aconteceu na semana passada?" ou "Classifique minhas conquistas por impressionabilidade." Em vez disso, √© uma base para melhor divis√£o e incorpora√ß√£o, garantindo que os peda√ßos recuperados mantenham o contexto de seus documentos de origem.

Pense no darkrag como uma base para construir agentes RAG mais avan√ßados, n√£o uma solu√ß√£o completa.

Ao focar na divis√£o mais inteligente, incorpora√ß√µes ricas em metadados e automa√ß√£o, esta configura√ß√£o melhora significativamente a precis√£o da recupera√ß√£o sem exigir APIs externas ou solu√ß√µes em nuvem.

### üîó C√≥digo-Fonte e Atualiza√ß√µes

*   GitHub: [darkrag](link_para_o_github_darkrag)

### O Que Vem a Seguir?

*   Estender a automa√ß√£o do n8n para atualiza√ß√µes de conhecimento em tempo real.
*   Melhorar as estrat√©gias de divis√£o e incorpora√ß√£o.
*   Adicionar e configurar agentes de IA.
